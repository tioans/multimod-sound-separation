{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5bc68c47-30dc-4c24-a1c1-bab099e1cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import torch\n",
    "from models import imagebind_model\n",
    "from models.imagebind_model import ModalityType\n",
    "\n",
    "import pickle, os, logging\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae8c1c7-8ad1-453a-bb42-e65db7f715a9",
   "metadata": {},
   "source": [
    "## Precompute embeddings from targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab33422-9566-47c0-aaca-b35af72ee46a",
   "metadata": {},
   "source": [
    "#### The embeddings are stored in a two level dict of shape: \n",
    "data_split(train/eval/test) -> list(tensor(n_samples, enc_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a65c11-1ec5-409b-8b17-73f4154d6d10",
   "metadata": {},
   "source": [
    "### Select project and data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec3cca55-96b0-4130-93e9-726eea4a6921",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = '/scratch/IOSZ/waveformer/multimod-sound-separation/multimod-waveformer'\n",
    "data_path = os.path.join(\"data\", \"CVSoundScapes\", \"cv-files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f584a-18a1-4b97-ace9-2f6e48eeb8bc",
   "metadata": {},
   "source": [
    "### Define methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53838888-4433-451c-8fa2-c53cddb8162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_folder(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "        \n",
    "\n",
    "def compute_embeddings(embedding_dict, data_split, project_path, targets, batch_size=10):\n",
    "    '''\n",
    "    Mehtod which computes embeddings for audio files in a given location (with an expected structure of one folder/label).\n",
    "    '''\n",
    "    \n",
    "    # target_list = []  # list to store all target embeddings\n",
    "    \n",
    "    # parse all label folders\n",
    "    for idx, target in enumerate(targets):\n",
    "                \n",
    "        tensor_list = []  # list to store all embeddings for this target\n",
    "        \n",
    "        # get all folders with data\n",
    "        target_folder_path = os.path.join(project_path, data_path, data_split, str(target))\n",
    "        \n",
    "        # get all files in a folder\n",
    "        file_paths = get_files_in_folder(target_folder_path)  \n",
    "         \n",
    "        # Create file batches\n",
    "        file_batches = [file_paths[i:i + batch_size] for i in range(0, len(file_paths), batch_size)]\n",
    "        \n",
    "        # parse all batches\n",
    "        for batch in file_batches:\n",
    "\n",
    "            # Load data\n",
    "            inputs = {\n",
    "                modality: data.load_and_transform_audio_data(batch, device)\n",
    "            }\n",
    "\n",
    "            # calculate embeddings\n",
    "            with torch.no_grad():\n",
    "                embedding = model(inputs)\n",
    "\n",
    "            # extends the list by appending elements from the specified iterable\n",
    "            # tensor_list.extend(embedding[ModalityType.AUDIO].cpu())\n",
    "            \n",
    "            # store embeddings in the dictionary\n",
    "            for file_path, emb in zip(batch, embedding[ModalityType.AUDIO].cpu()):\n",
    "                file_name = os.path.basename(file_path)\n",
    "                \n",
    "                if data_split not in embedding_dict:  # check for the data split\n",
    "                    embedding_dict[data_split] = {}\n",
    "                    \n",
    "                if file_name in embedding_dict[data_split]:\n",
    "                    print('File overriden: ', file_name)\n",
    "                \n",
    "                embedding_dict[data_split][file_name] = emb\n",
    "            \n",
    "            # release memory\n",
    "            del inputs\n",
    "            del embedding\n",
    "            \n",
    "        # stack the list into a single tensor and append it to the target list\n",
    "        # target_list.append(torch.stack(tensor_list))\n",
    "        \n",
    "        # del tensor_list\n",
    "        \n",
    "    # embedding_dict[data_split] = target_list  # assign the embeddings for the given data split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a53db2d-fd56-4a42-83d9-748f60f75fb1",
   "metadata": {},
   "source": [
    "### Load imagebind model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22763dbd-672d-4c0e-b957-3aafde062361",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3561a67f-630f-441c-b5f1-851a7d7a315a",
   "metadata": {},
   "source": [
    "### Select target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87f648ab-5968-4f8f-907f-a87c6da0a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = [\n",
    "    \"Acoustic_guitar\",\n",
    "    \"Applause\",\n",
    "    \"Bark\",\n",
    "    \"Bass_drum\",\n",
    "    \"Burping_or_eructation\",\n",
    "    \"Bus\",\n",
    "    \"Cello\",\n",
    "    \"Chime\",\n",
    "    \"Clarinet\",\n",
    "    \"Computer_keyboard\",\n",
    "    \"Cough\",\n",
    "    \"Cowbell\",\n",
    "    \"Double_bass\",\n",
    "    \"Drawer_open_or_close\",\n",
    "    \"Electric_piano\",\n",
    "    \"Fart\",\n",
    "    \"Finger_snapping\",\n",
    "    \"Fireworks\",\n",
    "    \"Flute\",\n",
    "    \"Glockenspiel\",\n",
    "    \"Gong\",\n",
    "    \"Gunshot_or_gunfire\",\n",
    "    \"Harmonica\",\n",
    "    \"Hi-hat\",\n",
    "    \"Keys_jangling\",\n",
    "    \"Knock\",\n",
    "    \"Laughter\",\n",
    "    \"Meow\",\n",
    "    \"Microwave_oven\",\n",
    "    \"Oboe\",\n",
    "    \"Saxophone\",\n",
    "    \"Scissors\",\n",
    "    \"Shatter\",\n",
    "    \"Snare_drum\",\n",
    "    \"Squeak\",\n",
    "    \"Tambourine\",\n",
    "    \"Tearing\",\n",
    "    \"Telephone\",\n",
    "    \"Trumpet\",\n",
    "    \"Violin_or_fiddle\",\n",
    "    \"Writing\",\n",
    "]\n",
    "\n",
    "\n",
    "TARGETS_CLEAN = [\n",
    "    \"Acoustic guitar\",\n",
    "    \"Applause\",\n",
    "    \"Bark\",\n",
    "    \"Bass drum\",\n",
    "    \"Burping or eructation\",\n",
    "    \"Bus\",\n",
    "    \"Cello\",\n",
    "    \"Chime\",\n",
    "    \"Clarinet\",\n",
    "    \"Computer keyboard\",\n",
    "    \"Cough\",\n",
    "    \"Cowbell\",\n",
    "    \"Double bass\",\n",
    "    \"Drawer open or close\",\n",
    "    \"Electric piano\",\n",
    "    \"Fart\",\n",
    "    \"Finger snapping\",\n",
    "    \"Fireworks\",\n",
    "    \"Flute\",\n",
    "    \"Glockenspiel\",\n",
    "    \"Gong\",\n",
    "    \"Gunshot or gunfire\",\n",
    "    \"Harmonica\",\n",
    "    \"Hi-hat\",\n",
    "    \"Keys jangling\",\n",
    "    \"Knock\",\n",
    "    \"Laughter\",\n",
    "    \"Meow\",\n",
    "    \"Microwave oven\",\n",
    "    \"Oboe\",\n",
    "    \"Saxophone\",\n",
    "    \"Scissors\",\n",
    "    \"Shatter\",\n",
    "    \"Snare drum\",\n",
    "    \"Squeak\",\n",
    "    \"Tambourine\",\n",
    "    \"Tearing\",\n",
    "    \"Telephone\",\n",
    "    \"Trumpet\",\n",
    "    \"Violin or fiddle\",\n",
    "    \"Writing\",\n",
    "]\n",
    "\n",
    "TARGETS_CV = [\n",
    "    \"female\",\n",
    "    \"male\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33793e6-0c21-4194-a791-ba5ab0eeee54",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8b9a777-ab4f-463b-91a8-2d4f387eb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a243bf2-30e6-4a81-b036-9be49f9d878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_dict = { 'train': {}, 'val': {}, 'test': {} }  # define initial structure\n",
    "embeddings_dict = {}\n",
    "\n",
    "modality = 'audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "261280e8-f1e4-48a4-a296-f83c68a99719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data split\n",
    "logger.setLevel(logging.ERROR)\n",
    "compute_embeddings(embeddings_dict, data_split='train', project_path=project_path, targets=TARGETS_CV)  # pass by reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5f59ad2-9b78-4f73-89af-ed54f8c19d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val data split\n",
    "logger.setLevel(logging.ERROR)\n",
    "compute_embeddings(embeddings_dict, data_split='val', project_path=project_path, targets=TARGETS_CV)  # pass by reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cf67b56-f9dd-426c-b017-a2204487d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data split\n",
    "logger.setLevel(logging.ERROR)\n",
    "compute_embeddings(embeddings_dict, data_split='test', project_path=project_path, targets=TARGETS_CV)  # pass by reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f64d19e6-9542-42ec-a5a5-81dfb52422cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_dict['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe9da1b-a4a8-48ba-a7a5-c31617e834c5",
   "metadata": {},
   "source": [
    "### Save the processed embeddings as a dict on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ae53f5a-35e2-4e0c-b994-dc89a90fd9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emb-imagebind-audio-cv-13-14-split-file-tens.pickle', 'wb') as handle:\n",
    "    pickle.dump(embeddings_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958299dd-5612-41be-b304-dc9c894116fa",
   "metadata": {},
   "source": [
    "## Testing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e0a66c-ce48-4b3a-b973-ed1ba562d460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_list = [torch.tensor([1,2,3],dtype=torch.float32), torch.tensor([2,3,4],dtype=torch.float32)]\n",
    "\n",
    "torch.stack(tensor_list, dim=0)\n",
    "\n",
    "tensor = torch.tensor([[1,2,3], [4,5,6]], dtype=torch.float32)\n",
    "tensor.shape\n",
    "\n",
    "torch.mean(torch.stack(tensor_list, dim=0), axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“sen“",
   "language": "python",
   "name": "sen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
