{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc68c47-30dc-4c24-a1c1-bab099e1cd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iosz/.conda/envs/sen/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/iosz/.conda/envs/sen/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/home/iosz/.conda/envs/sen/lib/python3.8/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/iosz/.conda/envs/sen/lib/python3.8/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "import torch\n",
    "from models import imagebind_model\n",
    "from models.imagebind_model import ModalityType\n",
    "\n",
    "import pickle, os, logging\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ace7a51a-c8c4-4cf5-b3fd-40cb27e8177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=[\"A dog\", \"A car\", \"A bird\"]\n",
    "image_paths=[\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
    "audio_paths=[\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22763dbd-672d-4c0e-b957-3aafde062361",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "print(device)\n",
    "\n",
    "# # # Load data\n",
    "# inputs = {\n",
    "#     ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "#     ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "#     ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "# }\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     embeddings = model(inputs)\n",
    "\n",
    "# print(\n",
    "#     \"Vision x Text: \",\n",
    "#     torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    "# )\n",
    "# print(\n",
    "#     \"Audio x Text: \",\n",
    "#     torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    "# )\n",
    "# print(\n",
    "#     \"Vision x Audio: \",\n",
    "#     torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d595ff83-4df1-47cb-a193-7a1ab192cb83",
   "metadata": {},
   "source": [
    "### Dump the model to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0772e7b8-4973-4727-8c93-de296bb8b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.cpu()\n",
    "\n",
    "# with open('imagebind.pickle', 'wb') as handle:\n",
    "#     pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3561a67f-630f-441c-b5f1-851a7d7a315a",
   "metadata": {},
   "source": [
    "### Select target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f648ab-5968-4f8f-907f-a87c6da0a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = [\n",
    "    \"Acoustic_guitar\",\n",
    "    \"Applause\",\n",
    "    \"Bark\",\n",
    "    \"Bass_drum\",\n",
    "    \"Burping_or_eructation\",\n",
    "    \"Bus\",\n",
    "    \"Cello\",\n",
    "    \"Chime\",\n",
    "    \"Clarinet\",\n",
    "    \"Computer_keyboard\",\n",
    "    \"Cough\",\n",
    "    \"Cowbell\",\n",
    "    \"Double_bass\",\n",
    "    \"Drawer_open_or_close\",\n",
    "    \"Electric_piano\",\n",
    "    \"Fart\",\n",
    "    \"Finger_snapping\",\n",
    "    \"Fireworks\",\n",
    "    \"Flute\",\n",
    "    \"Glockenspiel\",\n",
    "    \"Gong\",\n",
    "    \"Gunshot_or_gunfire\",\n",
    "    \"Harmonica\",\n",
    "    \"Hi-hat\",\n",
    "    \"Keys_jangling\",\n",
    "    \"Knock\",\n",
    "    \"Laughter\",\n",
    "    \"Meow\",\n",
    "    \"Microwave_oven\",\n",
    "    \"Oboe\",\n",
    "    \"Saxophone\",\n",
    "    \"Scissors\",\n",
    "    \"Shatter\",\n",
    "    \"Snare_drum\",\n",
    "    \"Squeak\",\n",
    "    \"Tambourine\",\n",
    "    \"Tearing\",\n",
    "    \"Telephone\",\n",
    "    \"Trumpet\",\n",
    "    \"Violin_or_fiddle\",\n",
    "    \"Writing\",\n",
    "]\n",
    "\n",
    "TARGETS_CLEAN = [\n",
    "    \"Acoustic guitar\",\n",
    "    \"Applause\",\n",
    "    \"Bark\",\n",
    "    \"Bass drum\",\n",
    "    \"Burping or eructation\",\n",
    "    \"Bus\",\n",
    "    \"Cello\",\n",
    "    \"Chime\",\n",
    "    \"Clarinet\",\n",
    "    \"Computer keyboard\",\n",
    "    \"Cough\",\n",
    "    \"Cowbell\",\n",
    "    \"Double bass\",\n",
    "    \"Drawer open or close\",\n",
    "    \"Electric piano\",\n",
    "    \"Fart\",\n",
    "    \"Finger snapping\",\n",
    "    \"Fireworks\",\n",
    "    \"Flute\",\n",
    "    \"Glockenspiel\",\n",
    "    \"Gong\",\n",
    "    \"Gunshot or gunfire\",\n",
    "    \"Harmonica\",\n",
    "    \"Hi-hat\",\n",
    "    \"Keys jangling\",\n",
    "    \"Knock\",\n",
    "    \"Laughter\",\n",
    "    \"Meow\",\n",
    "    \"Microwave oven\",\n",
    "    \"Oboe\",\n",
    "    \"Saxophone\",\n",
    "    \"Scissors\",\n",
    "    \"Shatter\",\n",
    "    \"Snare drum\",\n",
    "    \"Squeak\",\n",
    "    \"Tambourine\",\n",
    "    \"Tearing\",\n",
    "    \"Telephone\",\n",
    "    \"Trumpet\",\n",
    "    \"Violin or fiddle\",\n",
    "    \"Writing\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a068bd51-aa40-4aac-ae2b-ed0df0d55cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parse all labels and add to a dict\n",
    "# inputs = { ModalityType.TEXT: data.load_and_transform_text(TARGETS_CLEAN, device) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fd64514-75ce-4469-92e1-9c387100ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get embeddings\n",
    "# with torch.no_grad():\n",
    "#     embeddings = model(inputs)\n",
    "    \n",
    "# # put embeddings on cpu\n",
    "# #for key, val in ModalityType.__dict__.items():\n",
    "# embeddings[\"text\"] = np.array(embeddings[\"text\"].to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6363ab46-5d3a-43db-9096-9b1ef9583566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings[ModalityType.TEXT][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4be6aa6b-fc6d-477e-ad41-9c2d02edd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('emb-imagebind-orig.pickle', 'wb') as handle:\n",
    "#     pickle.dump(embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ec52841-35c1-43ef-a1c8-18d46730e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Load precomputed embeddings\n",
    "# with open('emb-imagebind-orig.pickle', 'rb') as handle:\n",
    "#     embeddings = pickle.load(handle)\n",
    "\n",
    "# embedding_mod = \"text\"\n",
    "# embedding_len = len(embeddings[\"text\"][0])  # get emb dim    \n",
    "\n",
    "# # Construct the query embedding vector\n",
    "# if len(args.targets) == 0:\n",
    "#     # case with no target\n",
    "#     query = torch.ones(1, len(embedding_len)) \n",
    "# elif len(args.targets) == 1:\n",
    "#     # case with single target\n",
    "#     query = embeddings[embedding_mod][TARGETS.index(t)]\n",
    "# else:\n",
    "#     # case with multiple targets\n",
    "#     emb_list = []\n",
    "#     for t in args.targets:\n",
    "#         emb_list.append(embeddings[embedding_mod][TARGETS.index(t)])\n",
    "    \n",
    "#     # average the embeddings\n",
    "#     query = np.mean(emb_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "427672d2-c947-4c07-ba95-4d5b934ec9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3]\n",
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 0, 0, 1], [0, 1, 1, 0]])\n",
    "\n",
    "# print(arr)\n",
    "\n",
    "for i, row in enumerate(arr):\n",
    "    print(np.where(row==1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4c1c196-d663-4b77-8822-e34b540bb51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 1, 1]), array([0, 3, 1, 2]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(arr==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1e6f0b8-398a-48b7-917c-c709fc31624b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a68879f6-d9b0-4c78-b1bb-cfc05bbb7716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1],\n",
       "       [0, 1, 1, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162086a8-7125-43b5-972b-3141d545179f",
   "metadata": {},
   "source": [
    "### Precompute embeddings from all audio targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53838888-4433-451c-8fa2-c53cddb8162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_folder(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "def compute_embeddings(embedding_dict, data_split, project_path):\n",
    "    \n",
    "    tensor_list = []\n",
    "    # train data path\n",
    "    for idx, target in enumerate(TARGETS):\n",
    "        target_folder_path = os.path.join(project_path, \"data\", \"FSDSoundScapes\", \"FSDKaggle2018\", data_split, str(target))\n",
    "\n",
    "        file_paths = get_files_in_folder(target_folder_path)  \n",
    "\n",
    "        # Load data\n",
    "        inputs = {\n",
    "            modality: data.load_and_transform_audio_data(file_paths, device)\n",
    "        }\n",
    "\n",
    "        # calculate embeddings\n",
    "        with torch.no_grad():\n",
    "            embedding = model(inputs)\n",
    "            \n",
    "        tensor_list.append(embedding[ModalityType.AUDIO])\n",
    "        \n",
    "    # embedding_dict[data_split] = torch.stack(tensor_list, dim=0)\n",
    "    \n",
    "    embedding_dict[data_split] = tensor_list\n",
    "    \n",
    "        # embedding_dict[data_split] = embedding[ModalityType.AUDIO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec3cca55-96b0-4130-93e9-726eea4a6921",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = '/scratch/IOSZ/waveformer/multimod-sound-separation/multimod-waveformer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab33422-9566-47c0-aaca-b35af72ee46a",
   "metadata": {},
   "source": [
    "#### The embeddings are stored in a two level dict of shape: \n",
    "data_split(train/eval/test) -> list(tensor(n_samples, enc_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8b9a777-ab4f-463b-91a8-2d4f387eb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a243bf2-30e6-4a81-b036-9be49f9d878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_dict = { 'train': {}, 'val': {}, 'test': {} }  # define initial structure\n",
    "embeddings_dict = {}\n",
    "\n",
    "modality = 'audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "261280e8-f1e4-48a4-a296-f83c68a99719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data split\n",
    "logger.setLevel(logging.ERROR)\n",
    "compute_embeddings(embeddings_dict, data_split='train', project_path=project_path)  # pass by reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5f59ad2-9b78-4f73-89af-ed54f8c19d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val data split\n",
    "logger.setLevel(logging.ERROR)\n",
    "compute_embeddings(embeddings_dict, data_split='val', project_path=project_path)  # pass by reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cf67b56-f9dd-426c-b017-a2204487d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data split\n",
    "logger.setLevel(logging.ERROR)\n",
    "compute_embeddings(embeddings_dict, data_split='test', project_path=project_path)  # pass by reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ae53f5a-35e2-4e0c-b994-dc89a90fd9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emb-imagebind-audio.pickle', 'wb') as handle:\n",
    "    pickle.dump(embeddings_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "116e4770-14d0-4a25-8287-27915d2bebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if str.lower(mode) == 'test':\n",
    "#     mode = 'eval'\n",
    "\n",
    "# curr_label = self.TARGETS[index]  # current label string\n",
    "# folder_path = os.path.join(os.getcwd(), \"data\", \"FSDSoundScapes\", \"FSDKaggle2018\", str.lower(mode), str(curr_label))\n",
    "# audio_path = os.path.join(folder_path, random.choice(os.listdir(folder_path)))  # choose one of the files\n",
    "\n",
    "# # print(\"AUDIO PATH: \", audio_path)\n",
    "\n",
    "# # Load data\n",
    "# inputs = {\n",
    "#     modality: data_imagebind.load_and_transform_audio_data([audio_path], self.device)\n",
    "# }\n",
    "\n",
    "# # calculate embeddings\n",
    "# with torch.no_grad():\n",
    "#     embedding = self.embedding_model(inputs)\n",
    "\n",
    "# return embedding[modality][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84c3999e-f916-4dc7-b340-fea31b7c3686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4710, -0.3620, -0.0745,  ...,  0.8085,  0.3394, -0.2203],\n",
       "        [-0.3242,  0.0505, -0.2391,  ...,  0.7286, -1.0235,  0.2464],\n",
       "        [-0.2584, -0.0451, -0.4842,  ...,  0.2111, -0.4052, -0.1214],\n",
       "        ...,\n",
       "        [-0.5626,  0.1492,  0.0880,  ...,  0.9239, -0.8681, -0.2784],\n",
       "        [-0.3582,  0.6139,  0.0278,  ...,  0.7972, -0.2738,  0.4377],\n",
       "        [-0.3770,  0.4972,  0.1245,  ...,  0.7261, -0.2226, -0.3311]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f318a74-1b73-4ec7-a578-db6ca3a00563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('emb-imagebind-audio.pickle', 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "tensor_list = [torch.tensor([1,2,3],dtype=torch.float32), torch.tensor([2,3,4],dtype=torch.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d41e687-ea25-40ef-8cae-a394b80f54bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [2., 3., 4.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(tensor_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e0a66c-ce48-4b3a-b973-ed1ba562d460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([[1,2,3], [4,5,6]], dtype=torch.float32)\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efc46a28-dc44-40b6-8dfe-82424a69a110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.5000, 3.5000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.stack(tensor_list, dim=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a529be-1155-4843-a7f7-5624964951cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“sen“",
   "language": "python",
   "name": "sen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
