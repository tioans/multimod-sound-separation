{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4da5bff-15c1-4553-a69b-a910c04c5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os, logging\n",
    "import numpy as np\n",
    "\n",
    "import torch as th\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0958a9df-59d1-4c32-99c5-a53de5e096be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Dir:  /scratch/IOSZ/waveformer/multimod-sound-separation\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "print(\"Root Dir: \", ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ba6aa-6cb2-4f5a-bf9b-4422d490014b",
   "metadata": {},
   "source": [
    "# Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07e24e31-cb95-431f-8ef0-1e87df45cb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir:  /scratch/IOSZ/waveformer/multimod-sound-separation/imagebind\n"
     ]
    }
   ],
   "source": [
    "# change the current path to the imagebind root\n",
    "os.chdir(os.path.join(ROOT_DIR, \"imagebind\"))\n",
    "print(\"Dir: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b286e6-181e-408e-bdfb-eed30f4ea76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iosz/.conda/envs/waveformer/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/iosz/.conda/envs/waveformer/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/home/iosz/.conda/envs/waveformer/lib/python3.8/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/iosz/.conda/envs/waveformer/lib/python3.8/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c351cf88-8ce9-43d8-b5a1-2b19f50b1791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if th.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "embedding_model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "embedding_model.eval()\n",
    "embedding_model.to(device)\n",
    "\n",
    "print(\"Embedding model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b175c2-6f3a-4ba1-8e18-6b4605f82584",
   "metadata": {},
   "source": [
    "# Waveformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e71192b2-b660-4c22-a330-76f1a263795b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/IOSZ/waveformer/multimod-sound-separation/multimod-waveformer/experiments/cv_dcc_tf_ckpt_E256_10_D128_3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveformer_path = os.path.join(ROOT_DIR, \"multimod-waveformer\", \"experiments\", \"cv_dcc_tf_ckpt_E256_10_D128_3\")\n",
    "model_ckp =  \"149.pt\"\n",
    "\n",
    "waveformer_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a645cc5c-af9a-494e-bdbb-4e39e7358295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir:  /scratch/IOSZ/waveformer/multimod-sound-separation/multimod-waveformer\n"
     ]
    }
   ],
   "source": [
    "# change the current path to the waveformer root\n",
    "os.chdir(os.path.join(ROOT_DIR, \"multimod-waveformer\"))\n",
    "print(\"Dir: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46010506-e279-430a-8ce0-d4160d6663d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers import utils\n",
    "from src.training.dcc_tf import Net as Waveformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0e50e47-df9e-47fa-aa91-4383e817d367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL LEN:  2\n",
      "ENCODING DIM:  256\n",
      "Audio model loaded!\n"
     ]
    }
   ],
   "source": [
    "# load config file\n",
    "params = utils.Params(os.path.join(waveformer_path, \"config.json\"))\n",
    "audio_model = Waveformer(**params.model_params)\n",
    "\n",
    "# Instantiate waveformer and load pretrained weights\n",
    "audio_model.load_state_dict(\n",
    "    th.load(os.path.join(waveformer_path, model_ckp), \n",
    "            map_location=device)[\"model_state_dict\"]\n",
    "    )\n",
    "\n",
    "audio_model.to(device).eval()\n",
    "\n",
    "print(\"Audio model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea3c73e-79db-43c0-9214-3ef91c820cce",
   "metadata": {},
   "source": [
    "# Process audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8a820c-72b6-411a-97ab-69d8c7a7f1dd",
   "metadata": {},
   "source": [
    "## Define file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9df3b9fc-0a12-4e00-8d3f-9770e28a789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose input, output and target\n",
    "INPUT_PATH = \"common_voice_en_36703188-extract.wav\"\n",
    "OUTPUT_PATH = \"common_voice_en_36703188-extract-extract.wav\"\n",
    "TARGET = \"Female voice\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f1942f-daca-4232-bf1a-fd66252246b7",
   "metadata": {},
   "source": [
    "## Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f4e2f20-ff89-48b7-bcd9-0d528f6351f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir:  /scratch/IOSZ/waveformer/multimod-sound-separation/imagebind\n"
     ]
    }
   ],
   "source": [
    "# change the current path to the imagebind root\n",
    "os.chdir(os.path.join(ROOT_DIR, \"imagebind\"))\n",
    "print(\"Dir: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8862f066-0e79-4417-88b9-0002961d82e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=[TARGET]\n",
    "audio_paths=[os.path.join(\"..\", INPUT_PATH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99535c79-f1a3-4aea-b505-03d85e6a712c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to open the input \"../common_voice_en_36703188-extract.wav\" (No such file or directory).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     ModalityType\u001b[38;5;241m.\u001b[39mTEXT: data\u001b[38;5;241m.\u001b[39mload_and_transform_text(text_list, device),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     ModalityType\u001b[38;5;241m.\u001b[39mAUDIO: \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_transform_audio_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embedding_model(inputs)\n",
      "File \u001b[0;32m/scratch/IOSZ/waveformer/multimod-sound-separation/imagebind/data.py:135\u001b[0m, in \u001b[0;36mload_and_transform_audio_data\u001b[0;34m(audio_paths, device, num_mel_bins, target_length, sample_rate, clip_duration, clips_per_video, mean, std)\u001b[0m\n\u001b[1;32m    130\u001b[0m clip_sampler \u001b[38;5;241m=\u001b[39m ConstantClipsPerVideoSampler(\n\u001b[1;32m    131\u001b[0m     clip_duration\u001b[38;5;241m=\u001b[39mclip_duration, clips_per_video\u001b[38;5;241m=\u001b[39mclips_per_video\n\u001b[1;32m    132\u001b[0m )\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m audio_path \u001b[38;5;129;01min\u001b[39;00m audio_paths:\n\u001b[0;32m--> 135\u001b[0m     waveform, sr \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_rate \u001b[38;5;241m!=\u001b[39m sr:\n\u001b[1;32m    137\u001b[0m         waveform \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mresample(\n\u001b[1;32m    138\u001b[0m             waveform, orig_freq\u001b[38;5;241m=\u001b[39msr, new_freq\u001b[38;5;241m=\u001b[39msample_rate\n\u001b[1;32m    139\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/waveformer/lib/python3.8/site-packages/torchaudio/backend/sox_io_backend.py:256\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fallback_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/waveformer/lib/python3.8/site-packages/torchaudio/io/_compat.py:107\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_audio\u001b[39m(\n\u001b[1;32m    100\u001b[0m     src: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    101\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mformat\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 107\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffmpeg_StreamReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_audio(s, frame_offset, num_frames, convert, channels_first)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to open the input \"../common_voice_en_36703188-extract.wav\" (No such file or directory)."
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    # ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with th.no_grad():\n",
    "    embeddings = embedding_model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad222c2-f6f4-42f7-b817-b1da9279b30f",
   "metadata": {},
   "source": [
    "## Run through waveformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad7fe393-87ac-457a-81fc-ce6a988319bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir:  /scratch/IOSZ/waveformer/multimod-sound-separation\n"
     ]
    }
   ],
   "source": [
    "# change the current path to the root\n",
    "os.chdir(ROOT_DIR)\n",
    "print(\"Dir: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba1ffd34-e290-4457-8d54-106d8892f832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded input audio from common_voice_en_36613135.wav\n",
      "Query shape:  torch.Size([1, 1024])\n",
      "Resample input to 44.1 kHz!\n",
      "Inference done. Saving output audio to common_voice_en_36613135-extract.wav\n"
     ]
    }
   ],
   "source": [
    "# Read input audio\n",
    "mixture, fs = torchaudio.load(INPUT_PATH)\n",
    "\n",
    "if fs != 44100:\n",
    "    mixture = torchaudio.functional.resample(mixture, orig_freq=fs, new_freq=44100)\n",
    "mixture = mixture.unsqueeze(0).to(device)\n",
    "print(\"Loaded input audio from %s\" % INPUT_PATH)\n",
    "\n",
    "# get the query from imagebind\n",
    "query = embeddings[ModalityType.TEXT]\n",
    "print(\"Query shape: \", query.shape)\n",
    "\n",
    "# run inference\n",
    "with th.inference_mode():\n",
    "    \n",
    "    # the fg_audio_paths arg here is not used, assuming the query has the right imagebind embeddings (1024)\n",
    "    output = audio_model(mixture.to(device), query.to(device), fg_audio_paths=query.to(device), mode=\"test\").squeeze(0).cpu()\n",
    "    \n",
    "    if fs != 44100:\n",
    "        output = torchaudio.functional.resample(output, orig_freq=44100, new_freq=fs)\n",
    "        print(\"Resample input to 44.1 kHz!\")\n",
    "    print(\"Inference done. Saving output audio to %s\" % OUTPUT_PATH)\n",
    "\n",
    "    assert not os.path.exists(OUTPUT_PATH), \"Output file already exists.\"\n",
    "    torchaudio.save(OUTPUT_PATH, output, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d68e98-7d0c-40ac-aac4-47c1c2fe66a9",
   "metadata": {},
   "source": [
    "## Inspect output file (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b594c14a-e25b-4873-92b4-de800688d370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22050\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Load audio file\n",
    "y, sr = librosa.load('common_voice_en_36613135-extract.wav')\n",
    "\n",
    "print(sr)\n",
    "\n",
    "# # Normalize audio data\n",
    "# y = librosa.util.normalize(y)\n",
    "\n",
    "# # Save normalized audio\n",
    "# librosa.output.write_wav('normalized_audio.wav', y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14903ae-a60b-4ad0-adb8-20ca503a5e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waveformer",
   "language": "python",
   "name": "waveformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
